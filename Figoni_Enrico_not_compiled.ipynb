{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy\n",
    "from scipy.stats import shapiro, ttest_1samp, ttest_ind, ttest_rel, f, bartlett, levene\n",
    "from scipy.optimize import minimize\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, explained_variance_score\n",
    "import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "from tabulate import tabulate\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (50,10)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['legend.fontsize'] = 'large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "# Introduction\n",
    "Code for the Introduction, with explorative analysis and the definitions of temp+ and temp-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "data = pd.read_excel('Thesis_Dataset.xlsx', sheet_name=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a copy of the dataset since the loading takes some time\n",
    "data_edit = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select a store\n",
    "to use as main example. To see the results for other stores, change the value in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "città = 'store_24' #this store is different from the one in used in the thesis so we can show different figures\n",
    "\n",
    "datacittà = data_edit[città].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we see the dataset for the store selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacittà"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some figures to better understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The consumes in kWh\n",
    "\n",
    "plt.plot(datacittà['kWh'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom on the consumes (example of January 2019)\n",
    "mask1 = datacittà['year']==2019\n",
    "mask2 = datacittà['month']==1\n",
    "plt.plot(datacittà[(mask1) & (mask2)]['kWh'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for the consumes and the datetime variables\n",
    "\n",
    "sns.pairplot(datacittà[['kWh', 'year', 'month', 'y_day', 'm_day']], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for the consumes, other datetime variables and the presence of customers and emplyees\n",
    "\n",
    "sns.pairplot(datacittà[['kWh', 'w_day', 'hour', 'cust', 'emp']], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the picture of the scatter plot of cust and emp for the thesis\n",
    "\n",
    "sns.pairplot(datacittà[['cust', 'emp']], kind=\"kde\")\n",
    "plt.savefig('cust_dip.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a picture of the ACF of cust\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(data_edit[città]['cust'].dropna(), ax = ax, lags=24*7)\n",
    "fig.savefig('cust_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a picture of the ACF of emp\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(data_edit[città]['emp'].dropna(), ax = ax, lags=24*7)\n",
    "fig.savefig('emp_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot for the consumes and the temperatures data\n",
    "\n",
    "sns.pairplot(datacittà[['kWh', 'temp', 'm_temp', 'M_temp']], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a picture of the scatter plot for the temperatures\n",
    "\n",
    "sns.pairplot(datacittà[['temp', 'm_temp', 'M_temp']], diag_kind=\"kde\")\n",
    "plt.savefig('temps.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF of the consumes\n",
    "\n",
    "sm.graphics.tsa.plot_acf(datacittà['kWh'].dropna(), lags=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF of temp\n",
    "\n",
    "sm.graphics.tsa.plot_acf(datacittà['temp'].dropna(), lags=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Studying the effect of temp on the consumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I collect all the temperatures registerd for the store (without duplicates)\n",
    "temp_list = np.sort(datacittà.dropna()['temp'].unique())\n",
    "\n",
    "# here I collect the maximum of the consumes for each temperature (remember that the temperature stays the same for each whole day)\n",
    "cons_list = []\n",
    "for temp in np.sort(temp_list):\n",
    "    cons = np.nanmax(datacittà[datacittà['temp'] == temp]['kWh'])\n",
    "    cons_list.append(cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we build a ball of influencial observation for the Savitzky-Golay filter (SG filter)\n",
    "ball = np.round(int(np.sqrt(len(cons_list))))\n",
    "if ball%2 !=1:\n",
    "    ball = ball+1\n",
    "ball = int(ball)\n",
    "\n",
    "# here we apply the SG filter on the maximum consumes collected above, with the \"ball\" just built and with a polynomial of degree 1\n",
    "poly_y = scipy.signal.savgol_filter(cons_list, ball, 1)\n",
    "\n",
    "# Save the values for t0 and c0 as defined in the thesis\n",
    "t0 = np.sort(datacittà['temp'].unique())[np.argmin(poly_y)]\n",
    "c0 = min(poly_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and plotting the results of the SG filter\n",
    "# in blu there are the maximum consumes per temperature\n",
    "# in orange the filtered consumes\n",
    "# in red t0\n",
    "# in green c0\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(temp_list, cons_list)\n",
    "ax.plot(temp_list, poly_y)\n",
    "\n",
    "ax.vlines(t0, 90,370, colors='red')\n",
    "ax.hlines(c0, -5, 35, colors='green')\n",
    "plt.xlabel('temp')\n",
    "plt.ylabel('max(kWh) per temp')\n",
    "fig.savefig('SG_filter.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding temp+ and temp-\n",
    "according to the definition given in the thesis.\n",
    "Temp is split in the 2 new variables and then we subtracted t0 and set =0 the negative values, like in a ReLU.\n",
    "We also eliminate the effect of the temperature on observations with consumes lower than c0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 2 new variables with center in t0\n",
    "datacittà['temp+'] = datacittà['temp']-t0\n",
    "datacittà['temp-'] = t0 - datacittà['temp']\n",
    "\n",
    "# creating the mask for the observations with temp+/- to be set =0\n",
    "maskt0p = datacittà['temp+']<0\n",
    "maskt0n = datacittà['temp-']<0\n",
    "maskc0 = datacittà['kWh']<c0\n",
    "\n",
    "# setting the =0s\n",
    "datacittà.loc[maskt0p, 'temp+']=0\n",
    "datacittà.loc[maskt0n, 'temp-']=0\n",
    "datacittà.loc[maskc0, 'temp+']=0\n",
    "datacittà.loc[maskc0, 'temp-']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scatter plot for the consumes, temp, temp+ and temp-\n",
    "\n",
    "sns.pairplot(datacittà[['kWh', 'temp', 'temp+', 'temp-']], diag_kind=\"kde\")\n",
    "plt.savefig('kWh_vs_temps.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating temp+ and temp- for all the stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same procedure seen above, this time inside a \"for cycle\" to set the variables for all the stores in the complete dataset\n",
    "for store in data_edit:\n",
    "    temp_list = np.sort(data_edit[store].dropna()['temp'].unique())\n",
    "    cons_list = []\n",
    "    for temp in np.sort(temp_list):\n",
    "        cons = np.nanmax(data_edit[store][data_edit[store]['temp'] == temp]['kWh'])\n",
    "        cons_list.append(cons)\n",
    "        \n",
    "    ball = np.round(int(np.sqrt(len(cons_list))))\n",
    "    if ball%2 !=1:\n",
    "        ball = ball+1\n",
    "    ball = int(ball)\n",
    "    poly_y = scipy.signal.savgol_filter(cons_list, ball, 1)\n",
    "\n",
    "    t0 = np.sort(data_edit[store]['temp'].unique())[np.argmin(poly_y)]\n",
    "    c0 = min(poly_y)\n",
    "    \n",
    "    data_edit[store]['temp+'] = data_edit[store]['temp']-t0\n",
    "    data_edit[store]['temp-'] = t0 - data_edit[store]['temp']\n",
    "\n",
    "    maskt0p = data_edit[store]['temp+']<0\n",
    "    maskt0n = data_edit[store]['temp-']<0\n",
    "    maskc0 = data_edit[store]['kWh']<c0\n",
    "\n",
    "    data_edit[store].loc[maskt0p, 'temp+']=0\n",
    "    data_edit[store].loc[maskt0n, 'temp-']=0\n",
    "    data_edit[store].loc[maskc0, 'temp+']=0\n",
    "    data_edit[store].loc[maskc0, 'temp-']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "# Models for explorative analysis\n",
    "Here we see the code for creating the models presented in this chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for the store selected at the beginning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the variables we want as X to build our model on them\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "\n",
    "# removing the variables from X that remains constant in the whole store dataset since they don't bring information\n",
    "X1 = []\n",
    "for x in X:\n",
    "    if len(datacittà[x].unique()) > 1:\n",
    "        X1.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x and y on which to build the model\n",
    "# y is kept as the original observations in kWh\n",
    "x = datacittà[X1]\n",
    "y = datacittà['kWh']\n",
    "\n",
    "# adding the intercept to x\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "# building the model thanks to the OLS function in statsmodels\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "\n",
    "# print the summary\n",
    "print(città, '\\n', results.summary(), '\\n\\n')\n",
    "\n",
    "# defining the fitted values for y\n",
    "yhat = results.fittedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the picture of y and yhat\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(yhat)\n",
    "plt.title('{}  -  R-squared: {}'.format(città, np.round(results.rsquared,2)))\n",
    "fig.savefig('OLS_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the ACF of the residuals of the model\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(y-yhat, ax = ax, lags=200)\n",
    "fig.savefig('OLS_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models for all the stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving some metrics to evaluate the models\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "# same process as above, this time for all the models\n",
    "for store in data_edit:\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "    \n",
    "    x = data_edit[store].dropna()[X1]\n",
    "    y = data_edit[store].dropna()['kWh']\n",
    "\n",
    "    x = sm.add_constant(x)\n",
    "\n",
    "    model = sm.OLS(y,x)\n",
    "    results = model.fit()\n",
    "    \n",
    "    \n",
    "    # saving the values for the metrics\n",
    "    R2.append(results.rsquared)\n",
    "    mse.append(mean_squared_error(y, results.fittedvalues))\n",
    "    expl_var.append(explained_variance_score(y, results.fittedvalues))\n",
    "\n",
    "# printing the median for the metrics\n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a boxplot of the R-squared obtained\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.boxplot(R2, labels=['OLS R-squared'], meanline=True, showmeans=True)\n",
    "fig.savefig('OLS_boxplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS model with transformations of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, but this time y = log(1+ kWh)\n",
    "x = datacittà[X1]\n",
    "y = np.log(1 + datacittà['kWh'])\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(città, '\\n', results.summary(), '\\n\\n')\n",
    "\n",
    "yhat = results.fittedvalues\n",
    "\n",
    "plt.plot(np.exp(y)-1)\n",
    "plt.plot(np.exp(yhat)-1)\n",
    "plt.title('{}'.format(città))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above, but this time y = sqrt(kWh)\n",
    "x = datacittà[X1]\n",
    "y = np.sqrt(datacittà['kWh'])\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y,x)\n",
    "results = model.fit()\n",
    "print(città, '\\n', results.summary(), '\\n\\n')\n",
    "\n",
    "yhat = results.fittedvalues\n",
    "\n",
    "plt.plot(y**2)\n",
    "plt.plot(yhat**2)\n",
    "plt.title('{}'.format(città))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNLS (Non Negative Least Square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we just show the results for all the stores\n",
    "\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "    \n",
    "    x = data_edit[store].dropna()[X1]\n",
    "    y = data_edit[store].dropna()['kWh']\n",
    "\n",
    "    x = sm.add_constant(x)\n",
    "    \n",
    "    # the model is built thanks to the command nnls in scipy\n",
    "    coef, res = scipy.optimize.nnls(x, y)\n",
    "    yhat = x.dot(coef)\n",
    "    \n",
    "    R2.append(r2_score(y, yhat))\n",
    "    mse.append(mean_squared_error(y, yhat))\n",
    "    expl_var.append(explained_variance_score(y, yhat))\n",
    "    \n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold Cross Validation to select the best value for the maximum number of leaves of thr DT\n",
    "\n",
    "num_leaves = range(10,100,10)\n",
    "scores = []\n",
    "for i in num_leaves:\n",
    "    i_scores = []\n",
    "    for store in data_edit:\n",
    "        X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "        X1 = []\n",
    "        for x in X:\n",
    "            if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "                X1.append(x)\n",
    "    \n",
    "        \n",
    "        x = data_edit[store].dropna()[X1]\n",
    "        y = data_edit[store].dropna()['kWh']\n",
    "        \n",
    "        model = DecisionTreeRegressor(max_leaf_nodes = i)\n",
    "        città_scores = cross_val_score(model, x, y, scoring='r2', cv=5)\n",
    "        città_score = np.nanmean(città_scores)\n",
    "        i_scores.append(città_score)\n",
    "    i_score = np.nanmean(i_scores)\n",
    "    scores.append(i_score)\n",
    "    \n",
    "mln = num_leaves[np.argmax(scores)]\n",
    "print(mln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving a picture of the R-squareds scored in the 5-fold CV\n",
    "# The red line is the maximum value\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(num_leaves, scores)\n",
    "ax.vlines(mln, 0.755, 0.76, color='red')\n",
    "fig.savefig('DT_kfCV.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the varaibles in X\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "X1 = []\n",
    "for x in X:\n",
    "    if len(datacittà.dropna()[x].unique()) > 1:\n",
    "        X1.append(x)\n",
    "\n",
    "# finding the time of the first intervention to split the dataset in training/test sets\n",
    "index0 = datetime.datetime(2020, 1, 1)\n",
    "for intervention in ['led', 'pvp', 'BMS']:\n",
    "    if len(datacittà.dropna()[intervention].unique()) > 1:\n",
    "        if index0 > datacittà[datacittà[intervention]==1].index[0]:\n",
    "            index0 = datacittà[datacittà[intervention]==1].index[0]\n",
    "\n",
    "# x, y -> training set\n",
    "# x1,y1 -> test set\n",
    "x = datacittà[datacittà.index<index0].dropna()[X1]\n",
    "x1 = datacittà[datacittà.index>=index0].dropna()[X1]\n",
    "y = datacittà[datacittà.index<index0].dropna()['kWh']\n",
    "y1 = datacittà[datacittà.index>=index0].dropna()['kWh']\n",
    "\n",
    "# building the model thanks to the command in sklearn\n",
    "model = DecisionTreeRegressor(max_leaf_nodes = mln) \n",
    "results = model.fit(x, y)\n",
    "\n",
    "yhat = results.predict(x)\n",
    "yhat = pd.Series(yhat, index=x.index)\n",
    "pred = results.predict(x1)\n",
    "pred = pd.Series(pred, index=x1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the plot\n",
    "# in blue the kWh pre first intervention\n",
    "# in orange the estimated kWh pre first intervention\n",
    "# in greem the kWh post first intervention\n",
    "# in red the estimated kWh post first intervention\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(yhat)\n",
    "ax.plot(y1)\n",
    "ax.plot(pred)\n",
    "plt.title('Decision Tree for {}'.format(città))\n",
    "fig.savefig('DT_plot_ex.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the tree\n",
    "\n",
    "fig= plt.figure(figsize=(50,50))\n",
    "plot_tree(model, feature_names=X, label='none', impurity=False, rounded=True, fontsize=25)\n",
    "fig.savefig('DT_tree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the ACF\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(y-yhat, ax = ax, lags=200)\n",
    "fig.savefig('DT_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again for all the stores\n",
    "\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    # setting X\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "    \n",
    "    # finding index0\n",
    "    index0 = datetime.datetime(2020, 1, 1)\n",
    "    for intervention in ['led', 'pvp', 'BMS']:\n",
    "        if len(data_edit[store].dropna()[intervention].unique()) > 1:\n",
    "            if index0 > data_edit[store][data_edit[store][intervention]==1].index[0]:\n",
    "                index0 = data_edit[store][data_edit[store][intervention]==1].index[0]\n",
    "\n",
    "    x = data_edit[store][data_edit[store].index<index0].dropna()[X1]\n",
    "    y = data_edit[store][data_edit[store].index<index0].dropna()['kWh']\n",
    "    \n",
    "    # 5-fold CV to select the maximum number of leaves\n",
    "    max_leaf_nodes = range(20,51,5)\n",
    "    parameters = {'max_leaf_nodes':max_leaf_nodes}\n",
    "    clf = GridSearchCV(DecisionTreeRegressor(), parameters, n_jobs=-1)\n",
    "    clf.fit(X=x, y=y)\n",
    "    \n",
    "    # building the model\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes = clf.best_params_['max_leaf_nodes'])\n",
    "    results = model.fit(x, y)\n",
    "\n",
    "    yhat = results.predict(x)\n",
    "    yhat = pd.Series(yhat, index=x.index)\n",
    "    \n",
    "    #evaluating the results on the training set\n",
    "    R2.append(results.score(x,y))\n",
    "    mse.append(mean_squared_error(y, yhat))\n",
    "    expl_var.append(explained_variance_score(y, yhat))\n",
    "    \n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting X\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "X1 = []\n",
    "for x in X:\n",
    "    if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "        X1.append(x)\n",
    "\n",
    "# finding index0\n",
    "for intervention in ['led', 'pvp', 'BMS']:\n",
    "    if len(datacittà.dropna()[intervention].unique()) > 1:\n",
    "        if index0 > datacittà[datacittà[intervention]==1].index[0]:\n",
    "            index0 = datacittà[datacittà[intervention]==1].index[0]\n",
    "\n",
    "x = datacittà[datacittà.index<index0].dropna()[X1]\n",
    "x1 = datacittà[datacittà.index>=index0].dropna()[X1]\n",
    "y = datacittà[datacittà.index<index0].dropna()['kWh']\n",
    "y1 = datacittà[datacittà.index>=index0].dropna()['kWh']\n",
    "\n",
    "# 5-fold CV for the number of trees and the maximum number of leaves in each tree\n",
    "n_estimators = range(1,52, 5)\n",
    "max_leaf_nodes = range(20,51,5)\n",
    "parameters = {'n_estimators':n_estimators, 'max_leaf_nodes':max_leaf_nodes}\n",
    "clf = GridSearchCV(RandomForestRegressor(), parameters, n_jobs=-1)\n",
    "clf.fit(X=x, y=y)\n",
    "print(clf.best_params_)\n",
    "\n",
    "# building the model thanks to the command again in sklearn\n",
    "model = RandomForestRegressor(n_estimators = clf.best_params_['n_estimators'], max_leaf_nodes = clf.best_params_['max_leaf_nodes'])\n",
    "results = model.fit(x, y)\n",
    "\n",
    "yhat = results.predict(x)\n",
    "yhat = pd.Series(yhat, index=x.index)\n",
    "pred = results.predict(x1)\n",
    "pred = pd.Series(pred, index=x1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the plot\n",
    "# colors as before\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(yhat)\n",
    "ax.plot(y1)\n",
    "ax.plot(pred)\n",
    "plt.title('Random Forest for {}'.format(città))\n",
    "fig.savefig('RF_plot_ex.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the ACF\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(y-yhat, ax = ax, lags=200)\n",
    "fig.savefig('RF_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again for all the stores\n",
    "\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1: #data_edit['store_'].dropna()\n",
    "            X1.append(x)\n",
    "    \n",
    "    index0 = datetime.datetime(2020, 1, 1)\n",
    "    for intervention in ['led', 'pvp', 'BMS']:\n",
    "        if len(data_edit[store].dropna()[intervention].unique()) > 1:\n",
    "            if index0 > data_edit[store][data_edit[store][intervention]==1].index[0]:\n",
    "                index0 = data_edit[store][data_edit[store][intervention]==1].index[0]\n",
    "\n",
    "    x = data_edit[store][data_edit[store].index<index0].dropna()[X1]\n",
    "    y = data_edit[store][data_edit[store].index<index0].dropna()['kWh']\n",
    "    \n",
    "    n_estimators = range(1,52, 5)\n",
    "    max_leaf_nodes = range(20,51,5)\n",
    "    parameters = {'n_estimators':n_estimators, 'max_leaf_nodes':max_leaf_nodes}\n",
    "    clf = GridSearchCV(RandomForestRegressor(), parameters, n_jobs=-1)\n",
    "    clf.fit(X=x, y=y)\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators = clf.best_params_['n_estimators'], max_leaf_nodes = clf.best_params_['max_leaf_nodes'])\n",
    "    results = model.fit(x, y)\n",
    "\n",
    "    yhat = results.predict(x)\n",
    "    yhat = pd.Series(yhat, index=x.index)\n",
    "    \n",
    "    \n",
    "    R2.append(results.score(x,y))\n",
    "    mse.append(mean_squared_error(y, yhat))\n",
    "    expl_var.append(explained_variance_score(y, yhat))\n",
    "    \n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Neighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'w_day', 'hour']\n",
    "X1 = []\n",
    "for x in X:\n",
    "    if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "        X1.append(x)\n",
    "\n",
    "# find index0\n",
    "index0 = datetime.datetime(2020, 1, 1)\n",
    "for intervention in ['led', 'pvp', 'BMS']:\n",
    "    if len(datacittà.dropna()[intervention].unique()) > 1:\n",
    "        if index0 > datacittà[datacittà[intervention]==1].index[0]:\n",
    "            index0 = datacittà[datacittà[intervention]==1].index[0]\n",
    "\n",
    "# set x,x1,y,y1 as defined also before\n",
    "# here we also add the dummies for w_day and hour\n",
    "x = datacittà[datacittà.index<index0].dropna()[X1]\n",
    "x = pd.get_dummies(x, prefix='w_day', columns=['w_day'])\n",
    "x = pd.get_dummies(x, prefix='hour', columns=['hour'])\n",
    "x1 = datacittà[datacittà.index>=index0].dropna()[X1]\n",
    "x1 = pd.get_dummies(x1, prefix='w_day', columns=['w_day'])\n",
    "x1 = pd.get_dummies(x1, prefix='hour', columns=['hour'])\n",
    "y = datacittà[datacittà.index<index0].dropna()['kWh']\n",
    "y1 = datacittà[datacittà.index>=index0].dropna()['kWh']\n",
    "\n",
    "# 5-fold CV to find the best value k for the closest neighbors to consider\n",
    "n_neighbors = range(30,101,10)\n",
    "parameters = {'n_neighbors':n_neighbors, 'weights':['distance']}\n",
    "clf = GridSearchCV(KNeighborsRegressor(), parameters, n_jobs=-1)\n",
    "clf.fit(X=x, y=y)\n",
    "\n",
    "# fitting the kNN model\n",
    "model = KNeighborsRegressor(clf.best_params_['n_neighbors'], weights = 'distance')\n",
    "results = model.fit(x, y)\n",
    "\n",
    "yhat = results.predict(x)\n",
    "yhat = pd.Series(yhat, index=x.index)\n",
    "pred = results.predict(x1)\n",
    "pred = pd.Series(pred, index=x1.index)\n",
    "\n",
    "\n",
    "# plotting the results\n",
    "plt.plot(y)\n",
    "plt.plot(yhat)\n",
    "plt.plot(y1)\n",
    "plt.plot(pred)\n",
    "plt.title('{}'.format(città))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving ACF\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(y-yhat, ax = ax, lags=200)\n",
    "fig.savefig('kNN_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN for all the stores (without dummies)\n",
    "\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "    \n",
    "    index0 = datetime.datetime(2020, 1, 1)\n",
    "    for intervention in ['led', 'pvp', 'BMS']:\n",
    "        if len(data_edit[store].dropna()[intervention].unique()) > 1:\n",
    "            if index0 > data_edit[store][data_edit[store][intervention]==1].index[0]:\n",
    "                index0 = data_edit[store][data_edit[store][intervention]==1].index[0]\n",
    "\n",
    "    x = data_edit[store][data_edit[store].index<index0].dropna()[X1]\n",
    "    y = data_edit[store][data_edit[store].index<index0].dropna()['kWh']\n",
    "    \n",
    "    n_neighbors = range(30,101,10)\n",
    "    parameters = {'n_neighbors':n_neighbors, 'weights':['distance']}\n",
    "    clf = GridSearchCV(KNeighborsRegressor(), parameters, n_jobs=-1)\n",
    "    clf.fit(X=x, y=y)\n",
    "\n",
    "    model = KNeighborsRegressor(clf.best_params_['n_neighbors'], weights = 'distance')\n",
    "    results = model.fit(x, y)\n",
    "\n",
    "    yhat = results.predict(x)\n",
    "    yhat = pd.Series(yhat, index=x.index)\n",
    "    \n",
    "    \n",
    "    R2.append(results.score(x,y))\n",
    "    mse.append(mean_squared_error(y, yhat))\n",
    "    expl_var.append(explained_variance_score(y, yhat))\n",
    "    \n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoRegressive\n",
    "Lag = 1, 2, 24, 7*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "X1 = []\n",
    "\n",
    "for x in X:\n",
    "    if len(datacittà[x].unique()) > 1:\n",
    "        X1.append(x)\n",
    "\n",
    "# find index0\n",
    "index0 = datetime.datetime(2020, 1, 1)\n",
    "for intervention in ['led', 'pvp', 'BMS']:\n",
    "    if len(datacittà.dropna()[intervention].unique()) > 1:\n",
    "        if index0 > datacittà[datacittà[intervention]==1].index[0]:\n",
    "            index0 = datacittà[datacittà[intervention]==1].index[0]\n",
    "\n",
    "# set model variables\n",
    "# we don't need an intercept because it's already included in the AutoReg model\n",
    "x = datacittà[datacittà.index<index0].dropna()[X1]\n",
    "x1 = datacittà[datacittà.index>=index0].dropna()[X1]\n",
    "y = datacittà[datacittà.index<index0].dropna()['kWh'].values\n",
    "y1 = datacittà[datacittà.index>=index0].dropna()['kWh']\n",
    "\n",
    "# fitting the model\n",
    "model = AutoReg(y, [1, 2, 24, 7*24], exog=x)\n",
    "results = model.fit()\n",
    "\n",
    "# estimates\n",
    "y=pd.Series(y)\n",
    "y.index = x.index\n",
    "yhat = results.fittedvalues\n",
    "pred = results.predict(start = len(y), end = len(y)+x1.shape[0]-1, exog_oos = x1)\n",
    "pred.index = y1.index\n",
    "\n",
    "\n",
    "# plotting the results\n",
    "plt.plot(y)\n",
    "plt.plot(yhat)\n",
    "plt.plot(y1)\n",
    "plt.plot(pred)\n",
    "plt.title('{}'.format(città))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the ACF\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(results.resid.dropna(), ax = ax, lags=200)\n",
    "fig.savefig('AR_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoReg for all the stores\n",
    "\n",
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "    X1 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "    \n",
    "    index0 = datetime.datetime(2020, 1, 1)\n",
    "    for intervention in ['led', 'pvp', 'BMS']:\n",
    "        if len(data_edit[store].dropna()[intervention].unique()) > 1:\n",
    "            if index0 > data_edit[store][data_edit[store][intervention]==1].index[0]:\n",
    "                index0 = data_edit[store][data_edit[store][intervention]==1].index[0]\n",
    "\n",
    "    x = data_edit[store][data_edit[store].index<index0].dropna()[X1]\n",
    "    y = data_edit[store][data_edit[store].index<index0].dropna()['kWh'].values\n",
    "    \n",
    "    model = AutoReg(y, [1, 2, 24, 7*24], exog=x)\n",
    "    results = model.fit()\n",
    "\n",
    "    y=pd.Series(y)\n",
    "    y.index = x.index\n",
    "    yhat = results.fittedvalues\n",
    "    y_true = y[168:]\n",
    "\n",
    "    \n",
    "    R2.append(r2_score(y_true, yhat))\n",
    "    mse.append(mean_squared_error(y_true, yhat))\n",
    "    expl_var.append(explained_variance_score(y_true, yhat))\n",
    "    \n",
    "print(np.median(R2), np.median(mse), np.median(expl_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cahpter 3\n",
    "# The State-Space Model\n",
    "Here we show the code for the main section of the thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kalman Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kfilter2(num,y,A,mu0,Sigma0,Phi,Ups,Gam,Theta,cQ,cR,S,u):\n",
    "    cQ = np.asmatrix(cQ)\n",
    "    Q = cQ.T.dot(cQ)\n",
    "    cR = np.asmatrix(cR)\n",
    "    R = cR.T.dot(cR)\n",
    "\n",
    "    Phi = np.asmatrix(Phi)\n",
    "    pdim = Phi.shape[0] \n",
    "    y = np.asmatrix(y).reshape((num,-1))\n",
    "    qdim = y.shape[1]\n",
    "    rdim = np.asmatrix(u).reshape((num,-1)).shape[1]\n",
    "    Ups = np.asmatrix(Ups).T\n",
    "    if (np.max(np.abs(Ups)) == 0):\n",
    "        Ups = np.zeros((pdim,rdim))\n",
    "    Gam = np.asmatrix(Gam)\n",
    "    if (np.max(np.abs(Gam)) == 0):\n",
    "        Gam = np.zeros((qdim,rdim))\n",
    "    ut = np.asmatrix(u).reshape((num,rdim))\n",
    "    xp = np.zeros((num,pdim,1)) # xp = x_t^{t-1}          \n",
    "    Pp = np.zeros((num,pdim,pdim)) # Pp = P_t^{t-1}\n",
    "    xf = np.zeros((num,pdim,1)) # xf = x_t^{t} \n",
    "    Pf = np.zeros((num,pdim,pdim)) # Pf = P_t^{t}\n",
    "    Gain = np.zeros((num,pdim,qdim))\n",
    "    innov = np.zeros((num,qdim,1)) # innovations\n",
    "    sig = np.zeros((num,qdim,qdim)) # innov var-cov matrix\n",
    "    like = 0                               # -log(likelihood)\n",
    "    xp[:][:][0] = Phi.dot(mu0) + Ups.dot(ut[0])   # mu1\n",
    "    Pp[:][:][0] = Phi.dot(Sigma0).dot(Phi.T)+Theta.dot(Q).dot(Theta.T)  #Sigma1\n",
    "    #print(Pp[:][:][0])\n",
    "    for i in range(num):\n",
    "        B = np.asmatrix(A[i])\n",
    "        innov[:][:][i] = y[i]-B.dot(xp[:][:][i])-Gam.dot(ut[i])\n",
    "        sigma = B.dot(Pp[:][:][i]).dot(B.T)+R \n",
    "        sigma = (sigma.T+sigma)/2     # make sure sig is symmetric\n",
    "        sig[:][:][i] = sigma\n",
    "        siginv = np.linalg.inv(sigma)\n",
    "        Gain[:][:][i] = (Phi.dot(Pp[:][:][i]).dot(B.T)+Theta.dot(S)).dot(siginv)\n",
    "        K = np.asmatrix(Gain[:][:][i])\n",
    "        xf[:][:][i] = xp[:][:][i] + Pp[:][:][i].dot(B.T).dot(siginv).dot(innov[:][:][i])\n",
    "        Pf[:][:][i] = Pp[:][:][i] - Pp[:][:][i].dot(B.T).dot(siginv).dot(B).dot(Pp[:][:][i])\n",
    "\n",
    "        like = like + np.float(np.log(np.linalg.det(sigma)) + innov[:][:][i].T.dot(siginv).dot(innov[:][:][i]))\n",
    "\n",
    "        if i != (num-1):\n",
    "            xp[:][:][i+1] = Phi.dot(xp[:][:][i]) + Ups.dot(ut[i+1,]) + K.dot(innov[:][:][i])\n",
    "            Pp[:][:][i+1] = Phi.dot(Pp[:][:][i]).dot(Phi.T)+ Theta.dot(Q).dot(Theta.T) - K.dot(sig[:][:][i]).dot(K.T)\n",
    "    \t \n",
    "    like = 0.5*like\n",
    "    return {'xp': xp,'Pp': Pp,'xf': xf,'Pf': Pf, 'K': Gain,'like': like,'innov': innov,'sig': sig}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Kalman Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ksmooth2(num,y,A,mu0,Sigma0,Phi,Ups,Gam,Theta,cQ,cR,S,u):\n",
    "    kf = Kfilter2(num,y,A,mu0,Sigma0,Phi,Ups,Gam,Theta,cQ,cR,S,u)\n",
    "    pdim = Phi.shape[0] \n",
    "    xs = np.zeros((num,pdim,1))\n",
    "    Ps = np.zeros((num,pdim,pdim))\n",
    "    J = np.zeros((num,pdim,pdim))\n",
    "    xs[:][:][-1] = kf['xf'][:][:][-1]\n",
    "    Ps[:][:][-1] = kf['Pf'][:][:][-1]\n",
    "    for k in range(num-1, 0, -1):\n",
    "        if np.linalg.det(kf['Pp'][:][:][k]) == 0:\n",
    "            J[:][:][k-1] = (kf['Pf'][:][:][k-1].dot(Phi.T)).dot(np.linalg.inv(kf['Pp'][:][:][k]+np.diag(np.repeat(1e-100, len(kf['Pp'][:][:][k])))))\n",
    "        else:\n",
    "            J[:][:][k-1] = (kf['Pf'][:][:][k-1].dot(Phi.T)).dot(np.linalg.inv(kf['Pp'][:][:][k]))\n",
    "        xs[:][:][k-1] = kf['xf'][:][:][k-1]+J[:][:][k-1].dot(xs[:][:][k]-kf['xp'][:][:][k])\n",
    "        Ps[:][:][k-1] = kf['Pf'][:][:][k-1]+J[:][:][k-1].dot(Ps[:][:][k]-kf['Pp'][:][:][k]).dot(J[:][:][k-1].T)\n",
    "    \n",
    "    return {'xs': xs, 'Ps': Ps, 'xp': kf['xp'],'Pp': kf['Pp'], 'xf': kf['xf'],'Pf': kf['Pf'], 'like': kf['like'], 'Kn':kf['K']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Monahan's reparameterisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Set_ARMA_parameters(vR, cp, cq):\n",
    "    vPhi   = np.array(np.repeat(0, cp)).reshape((cp,1))\n",
    "    vTheta = np.array(np.repeat(0, cq)).reshape((cq,1))\n",
    "    \n",
    "    if(cp>0):\n",
    "        vRR = np.array(np.repeat(0, cp)).reshape((cp,1))\n",
    "        for i in range(cp):\n",
    "            vRR[i] = vR[i] / np.sqrt(1.0 + vR[i] * vR[i])\n",
    "        vPhi[0] = vRR[0]\n",
    "        for k in range(1,cp):\n",
    "            vPhi[:k] = vPhi[:k] - vRR[k]*(vPhi[cp-k-1::-1])\n",
    "            vPhi[k] = vRR[k]\n",
    "    if(cq>0):\n",
    "        vRR = np.array(np.repeat(0, cq)).reshape((cq,1))\n",
    "        for i in range(cq):\n",
    "            vRR[i] = vR[cp+i] / np.sqrt(1.0 + vR[cp+i] * vR[cp+i])\n",
    "    vTheta[0] = vRR[0]\n",
    "    for k in range(1,cq):\n",
    "        vTheta[:k] = vTheta[:k] - vRR[k]*(vTheta[cq-k-1::-1])\n",
    "        vTheta[k] = vRR[k]\n",
    "        \n",
    "    return(np.append(vPhi,vTheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SSF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log-likelyhood function to optimize\n",
    "\n",
    "def Linn(para, y_data):\n",
    "    num = len(y_data)\n",
    "    alpha = para[0]\n",
    "    phi1 = para[1]\n",
    "    phi2 = para[2]\n",
    "    theta1 = para[3]\n",
    "    cQ1 = para[4]\n",
    "    cR = para[5]\n",
    "    \n",
    "    ARMA_parameters = [phi1, phi2, theta1]\n",
    "    ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "    phi1 = ARMA_parameters[0]\n",
    "    phi2 = ARMA_parameters[1]\n",
    "    theta1 = ARMA_parameters[2]\n",
    "    \n",
    "    Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "    for i in range(5):\n",
    "        Phi_diag.append(1)\n",
    "    Phi = np.diag(Phi_diag)\n",
    "    Phi[1,0] = phi2\n",
    "    Phi[0,1] = 1\n",
    "    Phi[2,3] = np.sin(lambda1)\n",
    "    Phi[3,2] = -np.sin(lambda1)\n",
    "    Phi[4,5] = np.sin(lambda2)\n",
    "    Phi[5,4] = -np.sin(lambda2)\n",
    "    \n",
    "    cQ = np.diag(np.repeat(cQ1, 11))\n",
    "    \n",
    "    Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "    for i in range(5):\n",
    "        Theta_diag.append(1)\n",
    "    Theta = np.diag(Theta_diag)\n",
    "    Theta[0,1] = Theta[1,0] = theta1\n",
    "    \n",
    "    S = np.array(np.repeat(0,11)).reshape((11,1))\n",
    "    mu0 = np.array(np.repeat(1,11)).reshape((11,1))\n",
    "    Sigma0 = np.diag(np.repeat(1, 11))\n",
    "    \n",
    "    u = np.repeat(1, num).reshape(1,num,1)\n",
    "    \n",
    "    kf = Kfilter2(num,y_data,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "    \n",
    "    return(kf['like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set X\n",
    "X = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "X1 = []\n",
    "\n",
    "for x in X:\n",
    "    if len(datacittà[x].unique()) > 1:\n",
    "        X1.append(x)\n",
    "\n",
    "# find index0\n",
    "index0 = datetime.datetime(2020, 1, 1)\n",
    "for intervention in ['led', 'pvp', 'BMS']:\n",
    "    if len(datacittà.dropna()[intervention].unique()) > 1:\n",
    "        if index0 > datacittà[datacittà[intervention]==1].index[0]:\n",
    "            index0 = datacittà[datacittà[intervention]==1].index[0]\n",
    "\n",
    "# set model variables\n",
    "x = datacittà[datacittà.index<index0].dropna()[X1]\n",
    "x1 = datacittà[datacittà.index>=index0].dropna()[X1]\n",
    "y = datacittà[datacittà.index<index0].dropna()['kWh'].values\n",
    "y1 = datacittà[datacittà.index>=index0].dropna()['kWh']\n",
    "\n",
    "\n",
    "# set some of the matrices for the SSF\n",
    "num = len(y)\n",
    "A1 = np.array(np.repeat(1, num)).reshape((num,1))\n",
    "A0 = np.array(np.repeat(0, num)).reshape((num,1))\n",
    "Ax = x\n",
    "A = np.hstack((A1,A0,A1,A0,A1,A0,Ax))\n",
    "u = np.repeat(1, num).reshape(1,num,1)\n",
    "\n",
    "lambda1 = (2*np.pi)/24\n",
    "lambda2 = (2*np.pi)/168\n",
    "\n",
    "# set the initial values for the parameters to optimize\n",
    "init_par = [-.77, .84, .83, .85, .12, 1.1]\n",
    "\n",
    "# find the best values for the parameters, remembering to correct the ARMA ones\n",
    "est = minimize(Linn, init_par, args=y, hess=True)\n",
    "\n",
    "alpha = est.x[0]\n",
    "phi1 = est.x[1]\n",
    "phi2 = est.x[2]\n",
    "theta1 = est.x[3]\n",
    "cQ1 = est.x[4]\n",
    "cR = est.x[5]\n",
    "\n",
    "ARMA_parameters = [phi1, phi2, theta1]\n",
    "ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "phi1 = ARMA_parameters[0]\n",
    "phi2 = ARMA_parameters[1]\n",
    "theta1 = ARMA_parameters[2]\n",
    "\n",
    "\n",
    "# set the remaining SSF matrices with the values just found through optimization\n",
    "Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "for i in range(5):\n",
    "    Phi_diag.append(1)\n",
    "Phi = np.diag(Phi_diag)\n",
    "Phi[1,0] = phi2\n",
    "Phi[0,1] = 1\n",
    "Phi[2,3] = np.sin(lambda1)\n",
    "Phi[3,2] = -np.sin(lambda1)\n",
    "Phi[4,5] = np.sin(lambda2)\n",
    "Phi[5,4] = -np.sin(lambda2)\n",
    "\n",
    "cQ = np.diag(np.repeat(cQ1, 11))\n",
    "\n",
    "Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "for i in range(5):\n",
    "    Theta_diag.append(1)\n",
    "Theta = np.diag(Theta_diag)\n",
    "Theta[0,1] = Theta[1,0] = theta1\n",
    "\n",
    "S = np.array(np.repeat(0,11)).reshape((11,1))\n",
    "mu0 = np.array(np.repeat(1,11)).reshape((11,1))\n",
    "Sigma0 = np.diag(np.repeat(1, 11))\n",
    "\n",
    "\n",
    "# fitting the model with the Kalman Filter\n",
    "kf = Kfilter2(num,y,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "\n",
    "# fitting the model with the Kalman Smoothing\n",
    "ks = Ksmooth2(num,y,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "\n",
    "# setting again y to have an index\n",
    "y = pd.Series(y)\n",
    "y.index = x.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finging the estimates of the KF\n",
    "y_star_f = np.repeat(0, num)\n",
    "for i in range(num):\n",
    "    if np.isnan(kf['xp'][i][0]):\n",
    "        y_star_f[i] = 0\n",
    "    else:\n",
    "        y_star_f[i] = A[i].dot(kf['xp'][i])+alpha\n",
    "\n",
    "y_star_f = pd.Series(y_star_f)\n",
    "y_star_f.index = y.index\n",
    "\n",
    "\n",
    "# saving the plot\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(y_star_f)\n",
    "plt.title('{} - y_star from Kfilter2'.format(città))\n",
    "fig.savefig('SSM_kf_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finging the estimates of the KS\n",
    "y_star_s = np.repeat(0, num)\n",
    "for i in range(num):\n",
    "    y_star_s[i] = A[i].dot(ks['xs'][i])+alpha\n",
    "\n",
    "y_star_s = pd.Series(y_star_s)\n",
    "y_star_s.index = y.index\n",
    "\n",
    "# saving the plot\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(y_star_s)\n",
    "plt.title('{} - y_star from Ksmooth2'.format(città))\n",
    "fig.savefig('SSM_ks_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the ACF\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "sm.graphics.tsa.plot_acf(y-y_star_s, ax = ax, lags=200)\n",
    "fig.savefig('SSM_acf.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating y1 with KS\n",
    "\n",
    "y1_star_s = np.repeat(0, len(y1))\n",
    "xs_star_s = []\n",
    "\n",
    "for i,x1_i in enumerate(x1.values):\n",
    "    if i == 0:\n",
    "        xs_star_s.append(Phi.dot(ks['xs'][-1]))\n",
    "    else:\n",
    "        xs_star_s.append(Phi.dot(np.array(xs_star_s[-1])))\n",
    "    A_i = np.array([1,0,1,0,1,0]+list(x1_i))\n",
    "    y1_star_s[i] = A_i.dot(xs_star_s[i])+alpha\n",
    "\n",
    "y1_star_s = pd.Series(y1_star_s)\n",
    "y1_star_s.index = y1.index\n",
    "xs_star_s = np.array(xs_star_s)\n",
    "\n",
    "# saving the plot\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(y)\n",
    "ax.plot(y_star_s)\n",
    "ax.plot(y1)\n",
    "ax.plot(y1_star_s)\n",
    "plt.title('{} - SSM smoothed with future predictions'.format(città))\n",
    "fig.savefig('SSM_ks_plot_wp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect the results for all the stores, with the same model we used just above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that includes most of the steps to get the estimates of the model\n",
    "\n",
    "def SSM_complete(y,x):\n",
    "    # setting some SSF matrices\n",
    "    num = len(y)\n",
    "    A1 = np.array(np.repeat(1, num)).reshape((num,1))\n",
    "    A0 = np.array(np.repeat(0, num)).reshape((num,1))\n",
    "    Ax = x\n",
    "    A = np.hstack((A1,A0,A1,A0,A1,A0,Ax))\n",
    "    u = np.repeat(1, num).reshape(1,num,1)\n",
    "\n",
    "    lambda1 = (2*np.pi)/24\n",
    "    lambda2 = (2*np.pi)/168\n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "    # redefining the log-likelyhood function at each repetion\n",
    "    def Linn(para, y_data):\n",
    "        alpha = para[0]\n",
    "        phi1 = para[1]\n",
    "        phi2 = para[2]\n",
    "        theta1 = para[3]\n",
    "        cQ1 = para[4]\n",
    "        cR = para[5]\n",
    "\n",
    "        ARMA_parameters = [phi1, phi2, theta1]\n",
    "        ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "        phi1 = ARMA_parameters[0]\n",
    "        phi2 = ARMA_parameters[1]\n",
    "        theta1 = ARMA_parameters[2]\n",
    "\n",
    "        Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "        for i in range(5):\n",
    "            Phi_diag.append(1)\n",
    "        Phi = np.diag(Phi_diag)\n",
    "        Phi[1,0] = phi2\n",
    "        Phi[0,1] = 1\n",
    "        Phi[2,3] = np.sin(lambda1)\n",
    "        Phi[3,2] = -np.sin(lambda1)\n",
    "        Phi[4,5] = np.sin(lambda2)\n",
    "        Phi[5,4] = -np.sin(lambda2)\n",
    "\n",
    "        cQ = np.diag(np.repeat(cQ1, 11))\n",
    "\n",
    "        Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "        for i in range(5):\n",
    "            Theta_diag.append(1)\n",
    "        Theta = np.diag(Theta_diag)\n",
    "        Theta[0,1] = Theta[1,0] = theta1\n",
    "\n",
    "        S = np.array(np.repeat(0,11)).reshape((11,1))\n",
    "        mu0 = np.array(np.repeat(1,11)).reshape((11,1))\n",
    "        Sigma0 = np.diag(np.repeat(1, 11))\n",
    "\n",
    "        kf = Kfilter2(num,y_data,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "\n",
    "        return(kf['like'])\n",
    "\n",
    "    ############################################################################################# \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # setting the initial values for the parameters and optimize them\n",
    "    init_par = [-.77, .84, .83, .85, .12, 1.1]\n",
    "    est = minimize(Linn, init_par, args=y)\n",
    "\n",
    "    alpha = est.x[0]\n",
    "    phi1 = est.x[1]\n",
    "    phi2 = est.x[2]\n",
    "    theta1 = est.x[3]\n",
    "    cQ1 = est.x[4]\n",
    "    cR = est.x[5]\n",
    "\n",
    "    ARMA_parameters = [phi1, phi2, theta1]\n",
    "    ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "    phi1 = ARMA_parameters[0]\n",
    "    phi2 = ARMA_parameters[1]\n",
    "    theta1 = ARMA_parameters[2]\n",
    "    \n",
    "    # setting the remaining SSF matrices with the optimized parameters\n",
    "    Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "    for i in range(5):\n",
    "        Phi_diag.append(1)\n",
    "    Phi = np.diag(Phi_diag)\n",
    "    Phi[1,0] = phi2\n",
    "    Phi[0,1] = 1\n",
    "    Phi[2,3] = np.sin(lambda1)\n",
    "    Phi[3,2] = -np.sin(lambda1)\n",
    "    Phi[4,5] = np.sin(lambda2)\n",
    "    Phi[5,4] = -np.sin(lambda2)\n",
    "\n",
    "    cQ = np.diag(np.repeat(cQ1, 11))\n",
    "\n",
    "    Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "    for i in range(5):\n",
    "        Theta_diag.append(1)\n",
    "    Theta = np.diag(Theta_diag)\n",
    "    Theta[0,1] = Theta[1,0] = theta1\n",
    "\n",
    "    S = np.array(np.repeat(0,11)).reshape((11,1))\n",
    "    mu0 = np.array(np.repeat(1,11)).reshape((11,1))\n",
    "    Sigma0 = np.diag(np.repeat(1, 11))\n",
    "    \n",
    "    # fitting the KS and computing the estimates\n",
    "    ks = Ksmooth2(num,y,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "    \n",
    "    y_star_s = np.repeat(0, num)\n",
    "    for i in range(num):\n",
    "        y_star_s[i] = A[i].dot(ks['xs'][i])+alpha\n",
    "\n",
    "    y_star_s = pd.Series(y_star_s)\n",
    "    y_star_s.index = y.index\n",
    "    \n",
    "    # returning the estimates\n",
    "    return y_star_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = []\n",
    "mse = []\n",
    "expl_var = []\n",
    "\n",
    "for store in data_edit:\n",
    "    # here we use X1 equals for all the stores, since it would be hard to change the log-likelyhood function for each different X1\n",
    "    X1 = ['error', 'cust', 'emp', 'temp+', 'temp-']\n",
    "    \n",
    "    # finding index0\n",
    "    index0 = datetime.datetime(2020, 1, 1)\n",
    "    for intervention in ['led', 'pvp', 'BMS']:\n",
    "        if len(data_edit[store].dropna()[intervention].unique()) > 1:\n",
    "            if index0 > data_edit[store][data_edit[store][intervention]==1].index[0]:\n",
    "                index0 = data_edit[store][data_edit[store][intervention]==1].index[0]\n",
    "\n",
    "    # setting the variables for the SSF model\n",
    "    x = data_edit[store][data_edit[store].index<index0].dropna()[X1].values\n",
    "    y = data_edit[store][data_edit[store].index<index0].dropna()['kWh']\n",
    "    \n",
    "    # usinig the previous function to compute the estimates\n",
    "    results = SSM_complete(y,x)\n",
    "    yhat = results\n",
    "    y_true = y\n",
    "    \n",
    "    #evaluating the results\n",
    "    R2.append(r2_score(y_true, yhat))\n",
    "    mse.append(mean_squared_error(y_true, yhat))\n",
    "    expl_var.append(explained_variance_score(y_true, yhat))\n",
    "    \n",
    "print(np.median(expl_var), np.median(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see the results inserted in Table 3.2.\n",
    "The model used here works with the interventions variables inserted in the regression part of the model (so without training/test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that includes most of the steps to get the state vectors' values\n",
    "\n",
    "def SSM_xs(y,x):\n",
    "    # setting some of the SSF matrices\n",
    "    m= len(x[0])\n",
    "    num = len(y)\n",
    "    A1 = np.array(np.repeat(1, num)).reshape((num,1))\n",
    "    A0 = np.array(np.repeat(0, num)).reshape((num,1))\n",
    "    Ax = x\n",
    "    A = np.hstack((A1,A0,A1,A0,A1,A0,Ax))\n",
    "    u = np.repeat(1, num).reshape(1,num,1)\n",
    "\n",
    "    lambda1 = (2*np.pi)/24\n",
    "    lambda2 = (2*np.pi)/168\n",
    "    \n",
    "    \n",
    "    ############################################################################################\n",
    "    # redefining the log-likelyhood function at each repetion\n",
    "    def Linn(para, y_data):\n",
    "        alpha = para[0]\n",
    "        phi1 = para[1]\n",
    "        phi2 = para[2]\n",
    "        theta1 = para[3]\n",
    "        cQ1 = para[4]\n",
    "        cR = para[5]\n",
    "\n",
    "        ARMA_parameters = [phi1, phi2, theta1]\n",
    "        ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "        phi1 = ARMA_parameters[0]\n",
    "        phi2 = ARMA_parameters[1]\n",
    "        theta1 = ARMA_parameters[2]\n",
    "\n",
    "        Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "        for i in range(m):\n",
    "            Phi_diag.append(1)\n",
    "        Phi = np.diag(Phi_diag)\n",
    "        Phi[1,0] = phi2\n",
    "        Phi[0,1] = 1\n",
    "        Phi[2,3] = np.sin(lambda1)\n",
    "        Phi[3,2] = -np.sin(lambda1)\n",
    "        Phi[4,5] = np.sin(lambda2)\n",
    "        Phi[5,4] = -np.sin(lambda2)\n",
    "\n",
    "        cQ = np.diag(np.repeat(cQ1, 6+m))\n",
    "\n",
    "        Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "        for i in range(m):\n",
    "            Theta_diag.append(1)\n",
    "        Theta = np.diag(Theta_diag)\n",
    "        Theta[0,1] = Theta[1,0] = theta1\n",
    "\n",
    "        S = np.array(np.repeat(0,6+m)).reshape((6+m,1))\n",
    "        mu0 = np.array(np.repeat(1,6+m)).reshape((6+m,1))\n",
    "        Sigma0 = np.diag(np.repeat(1, 6+m))\n",
    "\n",
    "        kf = Kfilter2(num,y_data,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "\n",
    "        return(kf['like'])\n",
    "\n",
    "    ############################################################################################# \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # setting the initial values for the parameters and optimize them\n",
    "    init_par = [-.77, .84, .83, .85, .12, 1.1]\n",
    "    est = minimize(Linn, init_par, args=y)\n",
    "\n",
    "    alpha = est.x[0]\n",
    "    phi1 = est.x[1]\n",
    "    phi2 = est.x[2]\n",
    "    theta1 = est.x[3]\n",
    "    cQ1 = est.x[4]\n",
    "    cR = est.x[5]\n",
    "\n",
    "    ARMA_parameters = [phi1, phi2, theta1]\n",
    "    ARMA_parameters = Set_ARMA_parameters(ARMA_parameters,2,1)\n",
    "    phi1 = ARMA_parameters[0]\n",
    "    phi2 = ARMA_parameters[1]\n",
    "    theta1 = ARMA_parameters[2]\n",
    "    \n",
    "    # setting the remaining SSF matrices with the optimized parameters\n",
    "    Phi_diag = [phi1,0,np.cos(lambda1),np.cos(lambda1),np.cos(lambda2),np.cos(lambda2)]\n",
    "    for i in range(m):\n",
    "        Phi_diag.append(1)\n",
    "    Phi = np.diag(Phi_diag)\n",
    "    Phi[1,0] = phi2\n",
    "    Phi[0,1] = 1\n",
    "    Phi[2,3] = np.sin(lambda1)\n",
    "    Phi[3,2] = -np.sin(lambda1)\n",
    "    Phi[4,5] = np.sin(lambda2)\n",
    "    Phi[5,4] = -np.sin(lambda2)\n",
    "\n",
    "    cQ = np.diag(np.repeat(cQ1, 6+m))\n",
    "\n",
    "    Theta_diag = [1,theta1**2,1,0,1,0]\n",
    "    for i in range(m):\n",
    "        Theta_diag.append(1)\n",
    "    Theta = np.diag(Theta_diag)\n",
    "    Theta[0,1] = Theta[1,0] = theta1\n",
    "\n",
    "    S = np.array(np.repeat(0,6+m)).reshape((6+m,1))\n",
    "    mu0 = np.array(np.repeat(1,6+m)).reshape((6+m,1))\n",
    "    Sigma0 = np.diag(np.repeat(1, 6+m))\n",
    "    \n",
    "    # fitting the KS\n",
    "    kf = Kfilter2(num,y,A,mu0,Sigma0,Phi,0,alpha,Theta,cQ,cR,S,u)\n",
    "    \n",
    "    # returning the state variables\n",
    "    return kf['xf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a very time consuming operation, so we check how much time does it take\n",
    "start_time = time.time()\n",
    "\n",
    "for store in data_edit:\n",
    "    # printing the store we are now considering\n",
    "    print(store)\n",
    "    \n",
    "    # again we check for the time\n",
    "    start_time_i = time.time()\n",
    "    \n",
    "    # setting X1 for the model and X2 for just the intervention(s) in the model\n",
    "    X = ['error', 'cust', 'emp', 'temp+', 'temp-', 'led', 'pvp', 'BMS']\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    for x in X:\n",
    "        if len(data_edit[store].dropna()[x].unique()) > 1:\n",
    "            X1.append(x)\n",
    "            if x in ['led', 'pvp', 'BMS']:\n",
    "                X2.append(x)\n",
    "    \n",
    "    # finding index0 for each intervention\n",
    "    index0_led = datetime.datetime(2020, 1, 1)\n",
    "    if len(data_edit[store].dropna()['led'].unique()) > 1:\n",
    "        if index0_led > data_edit[store][data_edit[store]['led']==1].index[0]:\n",
    "            index0_led = data_edit[store][data_edit[store]['led']==1].index[0]\n",
    "            \n",
    "    index0_pvp = datetime.datetime(2020, 1, 1)\n",
    "    if len(data_edit[store].dropna()['pvp'].unique()) > 1:\n",
    "        if index0_pvp > data_edit[store][data_edit[store]['pvp']==1].index[0]:\n",
    "            index0_pvp = data_edit[store][data_edit[store]['pvp']==1].index[0]\n",
    "            \n",
    "    index0_BMS = datetime.datetime(2020, 1, 1)\n",
    "    if len(data_edit[store].dropna()['BMS'].unique()) > 1:\n",
    "        if index0_BMS > data_edit[store][data_edit[store]['BMS']==1].index[0]:\n",
    "            index0_BMS = data_edit[store][data_edit[store]['BMS']==1].index[0]\n",
    "            \n",
    "    \n",
    "    # if we have no intervention the model is not even fitted, otherwise we fit the model and print the results\n",
    "    if len(X2)>0:\n",
    "        x = data_edit[store].dropna()[X1].values\n",
    "        y = data_edit[store].dropna()['kWh']\n",
    "\n",
    "        \n",
    "        # finding the estimates with the function defined just above and putting them in a DataFrame (instead of an array)\n",
    "        results = SSM_xs(y,x)\n",
    "        results = pd.DataFrame(results.squeeze(), index = y.index, columns = ['AR_1', 'AR_0', 'S1_1', 'S1_0', 'S2_1', 'S2_0']+X1)\n",
    "        \n",
    "        \n",
    "        # if an intervention happened, print the median and the mean of the relative state vector values for both pre and post intervention\n",
    "        if index0_led< datetime.datetime(2020, 1, 1):\n",
    "            print('\\n\\nled\\n')\n",
    "            print('median pre-interventions', np.median(results[results.index<index0_led]['led']))\n",
    "            print('mean pre-interventions', np.mean(results[results.index<index0_led]['led']))\n",
    "        \n",
    "            print('median post-interventions', np.median(results[results.index>=index0_led]['led']))\n",
    "            print('mean post-interventions', np.mean(results[results.index>=index0_led]['led']))\n",
    "        \n",
    "        if index0_pvp< datetime.datetime(2020, 1, 1):\n",
    "            print('\\npvp\\n')\n",
    "            print('median pre-interventions', np.median(results[results.index<index0_pvp]['pvp']))\n",
    "            print('mean pre-interventions', np.mean(results[results.index<index0_pvp]['pvp']))\n",
    "        \n",
    "            print('median post-interventions', np.median(results[results.index>=index0_pvp]['pvp']))\n",
    "            print('mean post-interventions', np.mean(results[results.index>=index0_pvp]['pvp']))\n",
    "            \n",
    "        if index0_BMS< datetime.datetime(2020, 1, 1):\n",
    "            print('\\nBMS\\n')\n",
    "            print('median pre-interventions', np.median(results[results.index<index0_BMS]['BMS']))\n",
    "            print('mean pre-interventions', np.mean(results[results.index<index0_BMS]['BMS']))\n",
    "        \n",
    "            print('median post-interventions', np.median(results[results.index>=index0_BMS]['BMS']))\n",
    "            print('mean post-interventions', np.mean(results[results.index>=index0_BMS]['BMS']))\n",
    "        \n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    # compute the time for each fitted model\n",
    "    end_time_i = time.time()\n",
    "    print(\"Modello {} in {} minuti.\\n\\n\\n \".format(store, int((end_time_i-start_time_i)/60+1)))\n",
    "    \n",
    "    \n",
    "# compute the time for fitting all the models\n",
    "end_time = time.time()\n",
    "print(\"Tutti i modelli in {} minuti.\\n \".format(int((end_time-start_time)/60+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
